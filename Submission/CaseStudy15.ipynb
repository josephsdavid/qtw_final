{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitminercondaa6c31f2afe9143f993ded22a64766581",
   "display_name": "Python 3.7.6 64-bit ('miner': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTW Lab 15\n",
    "\n",
    "## David Josephs, Andy Heroy, Carson Drake, Che' Cobb\n",
    "\n",
    "### April 10, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For the final case study for DS7333, we were given an unlabeled dataset and tasked with providing insight as to possible cost savings for a business partner. The dataset itself was 160,000 records with 51 unlabeled features.   Our only hint as to its domain is that its origins lie in the insurance industry.  The features themselves are labeled (x0-x49) with a binary target category ('y') of 0 or 1.  Our job is to apply machine learning modeling techniques to show a cost savings for our business partner for every incorrect classification of the target variable.  \n",
    "\n",
    "Seeing as we're dealing with a target variable that is binary, our job will be to select classification models that can best analyze the data.  The classification models we will attempt are as follows.  \n",
    "\n",
    "- Random Forrest\n",
    "- Random Forest with Permutation Importance\n",
    "- Random Forrest with PCA\n",
    "- Logistic Regression\n",
    "- ExtraDecisionTree's\n",
    "\n",
    "## Background\n",
    "\n",
    "Put simply, a classification model trains a model on a sample of training data to then predict the class of unseen test data.  The metrics for classification models are accuracy, precision, recall and F1-Score.  The main goal of this paper is to minimize false positives and false negatives.  As our business partner has kindly given us a \"cost\" function as to what false positives and negatives cost our company. \n",
    "\n",
    "- False positive = $ 10\n",
    "- False negative = $ 500\n",
    "- True pos/neg = $ 0\n",
    "\n",
    "When we evaluate our models, we implement a custom function to incorporate our business partners requirements in a cost function in the scoring section of the algorithm.  Sklearn comes with a handy function called make_scorer that allows us to monitor our models performance with our partners cost savings in mind.  For a basic overview of other metrics that are standard in classification.  See Table 1 below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metric | Description | Equation |\n",
    "|:---------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------:|\n",
    "| Accuracy | Accuracy is defined as the number of correct predictions divided  by the total number of predictions. | (True Positive + False Negatives)/Total amount of samples |\n",
    "| Precision | Precision is defined as the ratio of correctly predicted positives  to the total number of predicted positive observations. | True Positive/(True Positives + False Positives) |\n",
    "| Recall | Recall is defined as the ratio of correct positives to all the  observations in the class | True Positives/(True Positives + False Positives) |\n",
    "| F1-Score | F1 score is defined as the harmonic average of Precision and Recall.  This metric is more useful when you have uneven class balances but it  sometimes useful as it includes false postives and false negatives | 2x(Recall x Precision)/(Recall + Position) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation/Engineering\n",
    "\n",
    "\n",
    "Seeing as the data itself was unlabeled, we now begin the process of cleaning it in order to prepare it for modeling.  First, the target variable, `y`, is a categorical variable with two levels. Below are the steps and features engineered for this dataset.\n",
    "\n",
    "1.  Datatypes:\n",
    "    - Numerical: 45 features\n",
    "    - Categorical: 5 features\n",
    "    - Boolean: 1 feature\n",
    "2.  Correlation:\n",
    "    - Two sets of columns show direct correlation.  (Figure 2)\n",
    "    - x2 and x6\n",
    "    - x37 and x41\n",
    "    - Result: Due to perfect correlation the columns we decided to drop x2 and x41\n",
    "3.  Data Cleaning\n",
    "    - x24 contains country data.  \"euorpe\" was changed to \"europe.\"  We also one hot encoded it\n",
    "    - x29 contains monthly data.  We one hot encoded it.\n",
    "        - \"Dev\" was changed to \"Dec\"\n",
    "        - \"sept.\" was changed to \"Sep\"\n",
    "    - x30 contains day of the week.  \"thurday\" changed to \"thursday\".  We also one hot encoded it\n",
    "    - x32 contains a percentage amount. All % signs were removed and datatype changed to float64\n",
    "        - x32 was encoded as a categorical variable because it only had 5 unique levels.\n",
    "    - x37 contains a dollar amount.  All $ were removed and the datatype was changed to float64\n",
    "4.  Missing Value's \n",
    "    - Each column look to have anywhere from 21 - 47 missing values.  At most, this comprises about 2.9% of the data.  Which is a small amount when compared to the full dataset.  Instead of dropping those rows, we imputed with the mean for each numeric column.\n",
    "5.  Distribution\n",
    "    - After checking the histograms for each column, it was determined the data was normally distributed with no skew.  This leads us to believe that this dataset could have been generated with sklearns make_classification function.  To see the distributions check Figure 1 below. \n",
    "6.  Scaling\n",
    "    - In order to make sure our data was scaled correctly for classification, we implemented sklearns StandardScaler function over the numerical columns. \n",
    "7.  Categorical features\n",
    "    - The weekly and monthly categorical features were encoded as a sine and cosine with a weekly and monthly period, and an amplitude of two.   \n",
    "    - The continent variable was encoded in two ways, neither of which panned out, China or not china, and one hot encoded. All categorical variables were mode imputed, and all continuous variables were mean imputed. Mean imputation is appropriate because all the variables followed a normal distribution, as seen in the figure below:\n",
    "    \n",
    "![Histograms](../plots/Histograms.png)\n",
    "**Figure 1:** histograms of the numerical data\n",
    "Continuous distributions were tested both with and without scaling.\n",
    "\n",
    "![CorrelationPlot](../plots/Correlation_plot.png)\n",
    "**Figure 2:** Correlation plot of numerical categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Selection\n",
    "Feature selection took our group quite some time as having unlabeled columns makes it difficult to quantify what variables are important.  When this is the case, we rely on other methods to determine what columns are important for analysis and going back and forth of dropping them and running our analysis on various models. \n",
    "\n",
    "\n",
    "\n",
    "Our first attempt at this process was using Random Forest which comes with feature importance.  Upon the first run, we saw a fairly low variable scoring of importance within the dataset. Due to this, we decided to implement a \"Random\" column in to the analysis to see if random data was indeed more useful than the data.  Luckily, as you can see in the Figure 3 below, the random column ranks fairly low in the importance so we can rest a little easier knowing that our data contains useful information.  \n",
    "\n",
    "### Random Forest Feature Importance\n",
    "\n",
    "![FeatImportance](../plots/BaselineRF_Feature_Imp.png \"Baseline Random Forest Feature Importance\") **Figure 3:** Feature importance of baseline Random Forest\n",
    "\n",
    "\n",
    "\n",
    "The most important features according to Random Forest > 0.02 were as follows. \n",
    "\n",
    "    - x23\n",
    "    - x20\n",
    "    - x48\n",
    "    - x49\n",
    "    - x42\n",
    "    - x12\n",
    "    - x28\n",
    "    - x27\n",
    "    - x40\n",
    "    - x37\n",
    "    - x7 \n",
    "    - x46 \n",
    "    - x41\n",
    "    - x38\n",
    "    - x2 \n",
    "    - x6 \n",
    "    - x32\n",
    "\n",
    "Leaving the remaining 33 variables available for dropping in re-running a random forest.  We didn't immediately drop these columns because we wanted to run Principal Component Analysis (PCA) on the full dataset to discover how many columns we needed to keep to maintain at least 95% variance. This type of dimensionality technique is very useful when you have unlabeled data such as this.  \n",
    "\n",
    "### Principal Component Analysis\n",
    "\n",
    "The main beneft of PCA is to reduce the number of features when computational cost becomes too cumbersome when running a model.  Luckily, there's only 50 features for this dataset, but some can number in the 100's.  Below in Figure 4, we can see that in order to maintain 95% variance, we can select up to 36 different features to maintain our desired variance.  This is about twice as many features as with the random forest, but maintaining a conservative approach, we will use this limit to set our n_components=36 for the next random forest on the reduced dataset.  \n",
    "\n",
    "![PCA](../plots/PCA.png \"Principal Component Analysis\") **Figure 4:** PCA feature importance with respect to variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "Before we begin delving into classification models, its important to describe our loss function for this exercise as stipulated by our business partner.  \n",
    "\n",
    "#### Custom \"Slater-loss\" Function\n",
    "\n",
    "We now proceed with basic modeling of the data. The loss function is calculated as follows (given a confusion matrix $C$):\n",
    "$$\n",
    "\\mathbb{L}_\\mathrm{slater} =\\frac {C * \\begin{bmatrix}\n",
    "0 & 10  \\\\\n",
    "500 & 0  \\\\\n",
    "\\end{bmatrix}}  {\\sum_i \\sum_j C}\n",
    "$$\n",
    "Representing the cost in dollars per prediction. We want a dollar amount loss for every false positive/negative prediction. The business must earn more than that per prediction for this to be profitable.  We've come up with a way to evaluate the cost function using sklearns make_scorer [Make scorer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer).  To use this correctly, we will need to multiply the resulting confusion matrices by the weights we were given by Dr. Slater.  Below is the code for how this was performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    weight = np.array([[0, 10], [500, 0]])\n",
    "    out = cm * weight\n",
    "    return out.sum()/cm.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in the target columns true and predicted values, then multiplys the corresponding false positive/negatives by the dollar amount penalty for each.  It then divides the outcome by the sum of the confusion matrix to give us a dollar amount lost per incorrect prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Baseline Random Forest\n",
    "As was introduced earlier, our baseline random forest was run on our dataset after it had been cleaned and prepped for analysis in the data cleaning section. We used an 80-20 test split and stratified the target variable, using a 5 fold cross validation.  Our base prediction of the Random Forest Model built 300 tree's yielding quite good results as shown below.\n",
    "\n",
    "Baseline Random Forrest:\n",
    "Accuracy of Baseline RF: 92.49 %\n",
    "\n",
    "Confusion Matrix:\n",
    " \n",
    "| 18374 | 787 |\n",
    "|-------|-------|\n",
    "| 1617 | 11222 |\n",
    "\n",
    "Custom Cross Validation Score:\n",
    "\n",
    "(22.96359944 24.6265625  20.7203125  20.00625    23.20518831)\n",
    "\n",
    " Classification Report\n",
    "             \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      0.96      0.94     19161\n",
    "           1       0.93      0.87      0.90     12839\n",
    "\n",
    "    accuracy                           0.92     32000\n",
    "    macro avg      0.93      0.92      0.92     32000\n",
    "    weighted avg   0.93      0.92      0.92     32000\n",
    "\n",
    "As you can see from the custom cross validation scores, on average a false positive/negative only costs our business partner around $23.30 for each incorrect prediction.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with Permutation Importance\n",
    "\n",
    "Next, we used permutation importance in order to determine features which are important for generalization. An excellent discussion of permutation importance and other importance tools can be seen in [this blog post by fast.ai](https://explained.ai/rf-importance/) and [this blog post by the authors](https://josephsdavid.github.io/iml.html.  The basic mechanism is this.  The Random Forest will record a baseline accuracy by sending a validation set through model.  Then it permutes a single column and passes that column back to the test set.  Recomputes the accuracy and takes the difference of the two.  While expensive computationally, it can lead to an interesting insight as to which features are important in a classifier.\n",
    "\n",
    "Accuracy of RF w permutation importance: 93.28 %\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "| 18355 | 806 |\n",
    "|-------|-------|\n",
    "| 1345 | 11494 |\n",
    "\n",
    "Custom Cross Validation Score:\n",
    "\n",
    "(22.55929058 21.17351849 -19.50295614 -20.18493057 -20.40770024)\n",
    "\n",
    "Classification Report\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.93      0.96      0.94     19161\n",
    "           1       0.93      0.89      0.91     12839\n",
    "\n",
    "    accuracy                           0.93     32000\n",
    "    macro avg      0.93      0.93      0.92     32000\n",
    "    weighted avg   0.93      0.93      0.93     32000\n",
    "\n",
    "This model did very well showing our business partner a cost of \\$20.77 for each incorrect prediction. So far this model has the lead with both savings and accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with Prinicipal Component Analysis\n",
    "\n",
    "We introduced PCA in the feature engineering section of this case study but will now implement it. Another random forest was run with reduced dimensions set at n_components = 36.  As our earlier chart showed that would be a proper amount of features to select to maintain our goal of 95% variance.\n",
    "\n",
    "Accuracy of RF w PCA: 83.25 %\n",
    "Confusion Matrix:\n",
    "\n",
    "| 17496 | 1665 |\n",
    "|-------|------|\n",
    "| 3695 | 9144 |\n",
    "\n",
    "Custom Cross Validation Score:\n",
    "\n",
    "(33.12294954 33.0703125  28.484375   34.0953125  32.66760431)\n",
    "\n",
    "Classification Report\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.91      0.87     19161\n",
    "           1       0.85      0.71      0.77     12839\n",
    "\n",
    "    accuracy                           0.83     32000\n",
    "    macro avg      0.84      0.81      0.82     32000\n",
    "    weighted avg   0.83      0.83      0.83     32000\n",
    "\n",
    "Here we see the classification accuracy lower 9% points lower and the cost function yielding an average of \\$32.29 per wrong prediction.  Seeing as our business partner wants to save money rather than spend more, we don't want him/her to want to pay \\$10 more per wrong prediction.  We don't suggest dimensionality reduction to optimize cost savings.  \n",
    "\n",
    "\n",
    "Next, we implement a Logistic Regression Model as its another popular model for classification.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Now that we've seen a few models perform, lets turn our attention to logistic regression.  Our business partner has predicted that one might be able to achieve excellent accuracies with this particular model so we're interested in its implementation.  \n",
    "\n",
    "\n",
    "Baseline Logistic Regression:\n",
    "Accuracy of Logistic Regression: 70.44 %\n",
    "Confusion Matrix:\n",
    "\n",
    "| 1590 | 3260 |\n",
    "|------|------|\n",
    "| 6200 | 6639 |\n",
    "\n",
    "Custom Cross Validation Score:\n",
    "\n",
    "(97.47509863 97.85672435 98.18398437 96.64205633 99.56365483)\n",
    "\n",
    "Classification Report\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "           0       0.72      0.83      0.77     19161\n",
    "           1       0.67      0.52      0.58     12839\n",
    "\n",
    "    accuracy                           0.70     32000\n",
    "    macro avg      0.70      0.67      0.68     32000\n",
    "    weighted avg   0.70      0.70      0.70     32000\n",
    "\n",
    "\n",
    "Sadly our initial thoughts on this being an overly successful algorithm with the dataset is not working as expected.  Accuracy has dropped to 70.34% and on average, our business partner is losing almost \\$100 for every wrong prediction.  With that being said, Its probably safe to say that tuning a logistic regression at this point isn't going to recover 22% points in accuracy in order to catch up to Random Forest.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree's Classifier\n",
    "\n",
    "Lastly, we'll try implementing a Extra Tree's Classifer see if we can improve upon our baseline random forest.  The advantage of Extra Tree's is that this algorithm fits a number of randomized tree's on sub-samples of the dataset.  Using averaging to improve predictive accuracy and prevent over-fitting.  [Extra Tree's Docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)\n",
    "\n",
    "Baseline Extra Random Forest:\n",
    "Accuracy of Baseline ERF: 92.38 %\n",
    "\n",
    "Confusion Matrix: \n",
    "\n",
    "| 18473 | 688 |\n",
    "|-------|-------|\n",
    "| 1750 | 11089 |\n",
    "\n",
    "Custom Cross Validation Score:\n",
    "\n",
    "(30.3953125 32.35      25.8765625 27.7890625 29.534375)\n",
    "\n",
    "Classification Report\n",
    "\n",
    "                precision    recall  f1-score   support\n",
    "           0       0.91      0.96      0.94     19161\n",
    "           1       0.94      0.86      0.90     12839\n",
    "    accuracy                           0.92     32000\n",
    "    macro avg      0.93      0.91      0.92     32000\n",
    "    weighted avg   0.92      0.92      0.92     32000\n",
    "\n",
    "The Extra Tree's algorithm performed very well showing just 0.01% less accuracy than the base random forest.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Between all the models we've run, it Random Forest with permutation importance performed best barely beating out Extra Tree's Classifier by 0.8%.  Dimensionality reduction with PCA and another Random Forrest didn't pan out well for our purposes, while Logistic Regression came in last place in terms of accuracy and cost savings.  Therefore, our suggestion to our business partner is stick with Random Forest with Permutation Importance for their classification needs in cost savings.  \n",
    "\n",
    "| Model | Accuracy | Custom Scoring Loss |\n",
    "|-------------------------------------------|----------|---------------------|\n",
    "| Random Forest with Permutation Importance | 93.28% | $20.77 |\n",
    "| Random Forest | 92.49% | $23.30 |\n",
    "| Extra Tree's | 92.48% | $29.19 |\n",
    "| Random Forest with PCA | 83.25% | $32.29 |\n",
    "| Logistic Regression | 70.44% | $97.94 |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Appendix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from typing import Dict, Any\n",
    "from collections import Counter\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    weight = np.array([[0, 10], [500, 0]])\n",
    "    out = cm * weight\n",
    "    return out.sum() / cm.sum()  # y_true.shape[0]\n",
    "\n",
    "\n",
    "slater_loss = make_scorer(custom_loss, greater_is_better=False)\n",
    "\n",
    "\"\"\"\n",
    "data loading\n",
    "First we define some helper functions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    read a csv in from a dict\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    reader = csv.DictReader(open(path))\n",
    "    for row in reader:\n",
    "        for k, v in row.items():\n",
    "            result.setdefault(k, []).append(v)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _test_value(x: Any) -> bool:\n",
    "    \"\"\"\n",
    "    test if a column is continuous or not\n",
    "    \"\"\"\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def cleanup(d: Dict[str, str]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    turn default continuous columns into floats, replace non alphanumeric with np.nan\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for k, v in d.items():\n",
    "        if _test_value(v[0]):\n",
    "            res[k] = np.array([float(x) if _test_value(x) else np.nan for x in v])\n",
    "        else:\n",
    "            res[k] = np.array(v)\n",
    "    return res\n",
    "\n",
    "\n",
    "def convert_dollars_percs(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"replace dollar signs and percentages, as well as rogue negative signs\"\"\"\n",
    "    out = [\n",
    "        x[i].replace(\"$\", \"\").replace(\"%\", \"\").replace(\"-\", \"\")\n",
    "        for i in range(x.shape[0])\n",
    "    ]\n",
    "    out = np.array([float(z) if _test_value(z) else np.nan for z in out])\n",
    "    return out\n",
    "\n",
    "\n",
    "def impute_cats(\n",
    "    d: Dict[str, np.ndarray], c: Dict[str, Counter]\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    mode impute categorical variables, given a dictionary of counts and a\n",
    "    dictionary of data\n",
    "    \"\"\"\n",
    "    for k in c.keys():\n",
    "        d[k][np.isnan(d[k])] = c[k].most_common(1)[0][0]\n",
    "    return d\n",
    "\n",
    "\n",
    "def load_dataset(path: str) -> Dict[str, np.ndarray]:\n",
    "    data = load_data(path)\n",
    "    data = cleanup(data)\n",
    "    for k in [\"x32\", \"x37\"]:\n",
    "        data[k] = convert_dollars_percs(data[k])\n",
    "    # cats are continent, month, day, and percentage. We have those enumerated\n",
    "    # and mode imputed\n",
    "    cats = [k for k, v in data.items() if not _test_value(v[0])]\n",
    "    # percentage is also a categorical variable\n",
    "    cats.append(\"x32\")\n",
    "    # give it some nans\n",
    "    cont_dict = {\"\": np.nan}\n",
    "    for idx, k in enumerate(list(set(data[\"x24\"]))[1:]):\n",
    "        cont_dict[k] = idx\n",
    "    data[\"x24\"] = np.array([cont_dict[v] for v in data[\"x24\"]])\n",
    "    day_dict = dict(\n",
    "        zip([\"monday\", \"tuesday\", \"wednesday\", \"thurday\", \"friday\"], range(0, 5))\n",
    "    )\n",
    "    day_dict[\"\"] = np.nan\n",
    "    data[\"x30\"] = np.array([day_dict[v] for v in data[\"x30\"]])\n",
    "    months = [\n",
    "        \"January\",\n",
    "        \"Feb\",\n",
    "        \"Mar\",\n",
    "        \"Apr\",\n",
    "        \"May\",\n",
    "        \"Jun\",\n",
    "        \"July\",\n",
    "        \"Aug\",\n",
    "        \"sept.\",\n",
    "        \"Oct\",\n",
    "        \"Nov\",\n",
    "        \"Dev\",\n",
    "    ]\n",
    "    month_dict = dict(zip(months, range(0, 12)))\n",
    "    month_dict[\"\"] = np.nan\n",
    "    data[\"x29\"] = np.array([month_dict[v] for v in data[\"x29\"]])\n",
    "    cat_dict = {k: Counter(data[k]) for k in cats}\n",
    "    data = impute_cats(data, cat_dict)\n",
    "    return data\n",
    "\n",
    "\n",
    "def cyclical(x, period):\n",
    "    # http://blog.davidkaleko.com/feature-engineering-cyclical-features.html\n",
    "    \"\"\"\n",
    "    sine cosine transformation for days and months\n",
    "    \"\"\"\n",
    "    s = np.sin(x * (2.0 * np.pi / period))\n",
    "    c = np.cos(x * (2.0 * np.pi / period))\n",
    "    return s, c\n",
    "\n",
    "\n",
    "x = load_dataset(\"../data/final_project.csv\")\n",
    "sc = [cyclical(m, 5) for m in x[\"x30\"]]\n",
    "x[\"x30s\"] = np.stack(sc, axis=0)[:, 0]\n",
    "x[\"x30c\"] = np.stack(sc, axis=0)[:, 1]\n",
    "sc = [cyclical(m, 12) for m in x[\"x29\"]]\n",
    "x[\"x29s\"] = np.stack(sc, axis=0)[:, 0]\n",
    "x[\"x29c\"] = np.stack(sc, axis=0)[:, 1]\n",
    "\n",
    "# y variable\n",
    "y = x.pop(\"y\")\n",
    "\n",
    "\n",
    "# categorical variables with continent asia or not \n",
    "def categorize_with_asia(d: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    d = d.copy()\n",
    "    continent = \"x24\"\n",
    "    asia = Counter(d[continent]).most_common(1)[0][0]\n",
    "    d[continent] = np.array([1.0 if x == asia else 0.0 for x in d[continent]])\n",
    "    perc = \"x32\"\n",
    "    n_percs = np.unique(d[perc]).shape[0]\n",
    "    enum_dict = dict(zip(np.unique(d[perc]), range(np.unique(d[perc]).shape[0])))\n",
    "    enum_percs = np.array([enum_dict[x] for x in d[perc]])\n",
    "    d[perc] = np.eye(n_percs)[enum_percs]\n",
    "    to_drop = [\"x2\", \"x41\", \"x29\", \"x30\"]\n",
    "    for k in to_drop:\n",
    "        d.pop(k, None)\n",
    "    return d\n",
    "\n",
    "\n",
    "x_encoded = categorize_with_asia(x)\n",
    "X = np.column_stack(list(x_encoded.values()))\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "\n",
    "X = SimpleImputer().fit_transform(X)\n",
    "\n",
    "#Baseline Random Forest\n",
    "rfc_1 = RandomForestClassifier(n_estimators=300, n_jobs=-1, verbose=2)\n",
    "rfc_1_score = cross_val_score(\n",
    "    rfc_1, X, y, cv=5, scoring=slater_loss, n_jobs=-1, verbose=1\n",
    ")\n",
    "print(rfc_1_score)\n",
    "\n",
    "\n",
    "# happy matrix\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "rf = RandomForestClassifier(n_estimators=300, n_jobs=-1, verbose=2)\n",
    "rf.fit(X_train, y_train)\n",
    "ppp = rf.predict(X_test)\n",
    "print(confusion_matrix(y_test, ppp))\n",
    "\n",
    "\n",
    "def permutation_importances(rf, X_train, y_train, metric):\n",
    "    baseline = metric(rf, X_train, y_train)\n",
    "    imp = []\n",
    "    for i in range(X_train.shape[1]):\n",
    "        save = X_train[:, i].copy()\n",
    "        X_train[:, i] = np.random.permutation(X_train[:, i])\n",
    "        m = metric(rf, X_train, y_train)\n",
    "        X_train[:, i] = save\n",
    "        imp.append(baseline - m)\n",
    "    return np.array(imp)\n",
    "\n",
    "\n",
    "imps = permutation_importances(rf, X_train, y_train, slater_loss)\n",
    "\n",
    "for i in range(imps.shape[0]):\n",
    "    plt.barh(i, imps[i])\n",
    "plt.axvline(imps.mean())\n",
    "plt.show()\n",
    "\n",
    "keep_vars = [i for i in range(imps.shape[0]) if imps[i] > imps.mean()]\n",
    "\n",
    "\n",
    "X_small = X[:, keep_vars].copy()\n",
    "\n",
    "# Random forest with permutation importance:\n",
    "\n",
    "rfc_s = RandomForestClassifier(n_estimators=300, n_jobs=-1, verbose=2)\n",
    "rfc_s_score = cross_val_score(\n",
    "    rfc_s, X_small, y, cv=5, scoring=slater_loss, n_jobs=-1, verbose=1\n",
    ")\n",
    "print(rfc_s_score)\n",
    "\n",
    "\n",
    "# Random Forest scaled:\n",
    "\n",
    "rfc_s_scaled = RandomForestClassifier(n_estimators=300, n_jobs=-1, verbose=2)\n",
    "rfc_ss_score = cross_val_score(\n",
    "    rfc_s_scaled,\n",
    "    StandardScaler().fit_transform(X_small),\n",
    "    y,\n",
    "    cv=5,\n",
    "    scoring=slater_loss,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "print(rfc_ss_score)\n",
    "\n",
    "\n",
    "erfc_s = ExtraTreesClassifier(n_estimators=500, n_jobs=-1, verbose=2)\n",
    "erfc_s_score = cross_val_score(\n",
    "    erfc_s, X_small, y, cv=5, scoring=slater_loss, n_jobs=-1, verbose=1\n",
    ")\n",
    "print(erfc_s_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# logreg lives here\n",
    "from sklearn.metrics import mean_squared_error, r2_score, recall_score, confusion_matrix, make_scorer,classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "%time df = pd.read_csv(\"../data/final_project.csv\", sep=\",\", header=0)\n",
    "\n",
    "#Check ze rows\n",
    "print(len(df))\n",
    "print(df.shape)\n",
    "\n",
    "# %%\n",
    "df.info()\n",
    "df.describe()\n",
    "\n",
    "#%%\n",
    "#Looking at nulls\n",
    "df.isnull().sum()\n",
    "\n",
    "#%%\n",
    "#  Looking at missing data\n",
    "t_nulls = df.isnull().sum().sort_values(ascending=False)\n",
    "perc = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "missing_vals = pd.concat([t_nulls, perc], axis=1, keys=[\"Total\", \"Missing Percent\"])\n",
    "missing_vals[\"Missing Percent\"] = missing_vals['Missing Percent'].apply(lambda x: x *100)\n",
    "\n",
    "print(missing_vals)\n",
    "\n",
    "#%%\n",
    "\n",
    "#Looking at categorical.\n",
    "df_cat = df.describe(include=['object'])\n",
    "print(df_cat.T)\n",
    "\n",
    "#renaming columns\n",
    "df.rename(columns={'x24':'continent', 'x29':'month', 'x30':'day'},inplace=True)\n",
    "\n",
    "#%%\n",
    "# Data Cleaning.\n",
    "\n",
    "df['x37'] = df['x37'].str.replace('$', '').astype(float)\n",
    "df['x32'] = df['x32'].str.replace('%', '').astype(float)\n",
    "\n",
    "\n",
    "df['continent'] = df['continent'].str.replace('euorpe','europe')\n",
    "df['month'] = df['month'].str.replace('Dev','Dec')\n",
    "df['month'] = df['month'].str.replace('sept.','Sep')\n",
    "df['day'] = df['day'].str.replace('thurday','thursday')\n",
    "\n",
    "#Fill NA's with the median\n",
    "for col in df.select_dtypes(include=['float64']).columns:\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "#%%\n",
    "# EDA\n",
    "\n",
    "table=pd.crosstab(df['day'],df['y'])\n",
    "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Frequency of Target vs day of the week')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "table=pd.crosstab(df['month'],df['y'])\n",
    "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Frequency of Target vs month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "\n",
    "\n",
    "# Check numerical histograms of data\n",
    "df.hist(bins=50, figsize = (20,15))\n",
    "\n",
    "\n",
    "#%%\n",
    "# heatmap\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(df.corr().round(1),vmax=1, annot=True, cmap = 'YlGnBu',annot_kws={\"fontsize\":10})\n",
    "\n",
    "#%%\n",
    "X = df.drop('y', axis = 1)\n",
    "y = df['y']\n",
    "\n",
    "# Adding in a random noise componenet to test feature importance\n",
    "#X['Random'] = np.random.random(size=len(X))\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state = 42)\n",
    "\n",
    "print(\"\\nChecking shape of test/train data\")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "#Scaling Data and prepping for RF and PCA\n",
    "X1 = X_train.copy()\n",
    "y1 = y_train.copy()\n",
    "\n",
    "# Drop vars\n",
    "drop_col = ['day','month','continent', 'x2','x41']\n",
    "\n",
    "# Dropping from xtrain and xtest\n",
    "X1 = X1.drop(drop_col, axis=1)\n",
    "X_test_sc = X_test.drop(drop_col, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X1)\n",
    "X_test_sc = scaler.transform(X_test_sc)\n",
    "y1 = np.array(y1)\n",
    "\n",
    "#Setting up loss function\n",
    "def custom_loss(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    weight = np.array([[0, 10], [500, 0]])\n",
    "    out = cm * weight\n",
    "    return out.sum()/cm.sum()\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "# Baseline RF\n",
    "\n",
    "#Fit the model\n",
    "slater_loss = make_scorer(custom_loss, greater_is_better=True)\n",
    "\n",
    "#%%\n",
    "# PCA with no components\n",
    "pca = PCA().fit(X_train_sc)\n",
    "pca_trans = pca.transform(X_train_sc)\n",
    "\n",
    "print(\"\\nThe components are as follows:\\n {}\".format(pca.components_))\n",
    "print(\"\\nThe explained variance is :\\n {}\".format(pca.explained_variance_))\n",
    "\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title(\"PCA Analysis\")\n",
    "plt.xlabel('number of components',fontsize=15)\n",
    "plt.ylabel('cumulative explained variance', fontsize=15)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.axvline(linewidth=3, color='r', linestyle='--', x = 36, ymin=0)\n",
    "plt.axhline(y=0.95, xmin=0, color='r', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "#%%\n",
    "# PCA with 36 components.  Which retains 95% of the variation\n",
    "pca = PCA(n_components=36).fit(X_train_sc)\n",
    "X_train_sc_pca = pca.transform(X_train_sc)\n",
    "X_test_sc_pca = pca.transform(X_test_sc)\n",
    "\n",
    "# Now rfc on the reduced data\n",
    "rfc_2 = RandomForestClassifier(n_estimators=300, n_jobs=-1, verbose=2)\n",
    "%time rfc_2.fit(X_train_sc_pca, y1)\n",
    "\n",
    "y_pred_pca = rfc_2.predict(X_test_sc_pca)\n",
    "\n",
    "# Custom Loss function\n",
    "slater_loss= make_scorer(custom_loss, greater_is_better=True)\n",
    "\n",
    "rfc_2_cf = confusion_matrix(y_test, y_pred_pca)\n",
    "rfc_2_score = cross_val_score(rfc_2, X_test_sc_pca, y_pred_pca, cv=5, scoring=slater_loss)\n",
    "\n",
    "print(\"\\nRandom Forest w PCA:\")\n",
    "print('Accuracy of RF w PCA: {:.2f}'.format(rfc_2.score(X_test_sc_pca, y_test)*100),'%')\n",
    "print(\"Confusion Matrix:\\n\",rfc_2_cf )\n",
    "print(\"Custom Cross Validation Score:\\n\", rfc_2_score)\n",
    "print(\"Classification Report\", classification_report(y_test, y_pred_pca))\n",
    "\n",
    "\n",
    "# %%\n",
    "#Log reg\n",
    "\n",
    "#Reimporting the dat\n",
    "X = df.drop('y', axis = 1)\n",
    "y = df['y']\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state = 42)\n",
    "\n",
    "X1_train = X_train.copy()\n",
    "y1_train = y_train.copy()\n",
    "\n",
    "\n",
    "# Drop vars\n",
    "drop_col = ['day','month','continent', 'x2','x41']\n",
    "\n",
    "# Dropping from xtrain and xtest\n",
    "X1_train = X1_train.drop(drop_col, axis=1)\n",
    "X1_test = X_test.drop(drop_col, axis=1)\n",
    "\n",
    "#Scaling\n",
    "scaler = StandardScaler()\n",
    "X1_train_sc = scaler.fit_transform(X1_train)\n",
    "X1_test_sc = scaler.transform(X1_test)\n",
    "y1_train = np.array(y1_train)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    weight = np.array([[0, 10], [500, 0]])\n",
    "    out = cm * weight\n",
    "    return out.sum()/cm.sum()\n",
    "\n",
    "# Logistic Regression\n",
    "lr_1 = LogisticRegression(penalty='l2')\n",
    "lr_1.fit(X1_train_sc, y1_train)\n",
    "y_pred = lr_1.predict(X1_test_sc)\n",
    "\n",
    "slater_loss = make_scorer(custom_loss, greater_is_better=True)\n",
    "lr_1_score = cross_val_score(lr_1,\n",
    "\t\t\t\t\t\t\tStandardScaler().fit_transform(X1),\n",
    "\t\t\t\t\t\t\ty1,\n",
    "\t\t\t\t\t\t\tcv=5,\n",
    "\t\t\t\t\t\t\tscoring = slater_loss,\n",
    "\t\t\t\t\t\t\tn_jobs=-1,\n",
    "\t\t\t\t\t\t\tverbose=1)\n",
    "\n",
    "lr_confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Baseline Logistic Regression:\")\n",
    "print('Accuracy of Logistic Regression: {:.2f}'.format(lr_1.score(X1_test_sc, y_test)*100),'%')\n",
    "print(\"Confusion Matrix:\\n\", lr_confusion)\n",
    "print(\"Custom Cross Validation Score:\\n\", lr_1_score)\n",
    "print(\"Classification Report\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  }
 ]
}