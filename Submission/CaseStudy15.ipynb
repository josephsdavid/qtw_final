{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitminercondaa6c31f2afe9143f993ded22a64766581",
   "display_name": "Python 3.7.6 64-bit ('miner': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTW Lab 15\n",
    "\n",
    "## David Josephs, Andy Heroy, Carson Drake, Che' Cobb\n",
    "\n",
    "### April 10, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For the final case study for DS7333, we were given an unlabeled dataset and tasked with providing insight as to possible cost savings for a business partner. The dataset itself was 160,000 records with 51 unlabeled features.   Our only hint as to its domain is that its origins lie in the insurance industry.  The features themselves are labeled (x0-x49) with a binary target category ('y') of 0 or 1.  Our job is to apply machine learning modeling techniques to show a cost savings for our business partner for every incorrect classification of the target variable.  \n",
    "\n",
    "Seeing as we're dealing with a target variable that is binary, our job will be to select classification models that can best analyze the data.  The classification models we will attempt are as follows.  \n",
    "\n",
    "- Random Forrest\n",
    "- Random Forest with Permutation Importance\n",
    "- Random Forrest with PCA\n",
    "- Logistic Regression\n",
    "- ExtraDecisionTree's\n",
    "\n",
    "## Background\n",
    "\n",
    "Put simply, a classification model trains a model on a sample of training data to then predict the class of unseen test data.  The metrics for classification models are accuracy, precision, recall and F1-Score.  The main goal of this paper is to minimize false positives and false negatives.  As our business partner has kindly given us a \"cost\" function as to what false positives and negatives cost our company. \n",
    "\n",
    "- False positive = $ 10\n",
    "- False negative = $ 500\n",
    "- True pos/neg = $ 0\n",
    "\n",
    "When we evaluate our models, we implement a custom function to incorporate our business partners requirements in a cost function in the scoring section of the algorithm.  Sklearn comes with a handy function called make_scorer that allows us to monitor our models performance with our partners cost savings in mind.  For a basic overview of other metrics that are standard in classification.  See Table 1 below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metric | Description | Equation |\n",
    "|:---------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------:|\n",
    "| Accuracy | Accuracy is defined as the number of correct predictions divided  by the total number of predictions. | (True Positive + False Negatives)/Total amount of samples |\n",
    "| Precision | Precision is defined as the ratio of correctly predicted positives  to the total number of predicted positive observations. | True Positive/(True Positives + False Positives) |\n",
    "| Recall | Recall is defined as the ratio of correct positives to all the  observations in the class | True Positives/(True Positives + False Positives) |\n",
    "| F1-Score | F1 score is defined as the harmonic average of Precision and Recall.  This metric is more useful when you have uneven class balances but it  sometimes useful as it includes false postives and false negatives | 2x(Recall x Precision)/(Recall + Position) |\n",
    "\n",
    "The equations for these metrics can be seen below:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{acc} = \\left(\\mathop{\\sum_{j}\\sum_{i}}_{i=j} M\\right) / \\left(\\mathop{\\sum_{j}\\sum_{i}} M\\right)\\\\\n",
    "\\mathrm{precision} = \\mathrm{diag}(M) / \\left(\\mathop{\\sum_{j}} M\\right)\\\\\n",
    "\\mathrm{recall} = \\mathrm{diag}(M) / \\left(\\mathop{\\sum_{i}} M\\right)\\\\\n",
    "\\mathrm{f1} = \\frac{2(\\mathrm{precision})(\\mathrm{recall})}{\\mathrm{precision} + \\mathrm{recall}}\n",
    "\\end{align}\n",
    "\n",
    "## Data Preparation\n",
    "The data is completely unlabeled, so we had to figure out on our own what each variable meant. First, the target variable, `y`, is a categorical variable with two levels. There are also 45 continuous features, and 5 categorical features. Two pairs of features, `x2` and `x6`, and `x37` and `x41`, have 100% perfect correlation, thus one of their partners were removed. The weekly and monthly categorical features were encoded as a sine and cosine with a weekly and monthly period, and an amplitude of two. The percentage variable, `x32`, was encoded as a categorical variable because it only had 5 unique levels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "\n",
    "Seeing as the data itself was unlabeled, we now begin the process of cleaning it in order to prepare it for modeling.  Below are our findings of the various properties of the dataset.\n",
    "\n",
    "1.  Datatypes:\n",
    "    - Numerical: 45 features\n",
    "    - Categorical: 5 features\n",
    "    - Boolean: 1 feature\n",
    "2.  Correlation:\n",
    "    - Two sets of columns show direct correlation.  \n",
    "    - x2 and x6\n",
    "    - x37 and x41\n",
    "    - Result: Due to perfect correlation the columns we decided to drop x2 and x41\n",
    "3.  Data Cleaning\n",
    "    - x24 contains country data.  \"euorpe\" was changed to \"europe\"\n",
    "    - x29 contains monthly data.\n",
    "        - \"Dev\" was changed to \"Dec\"\n",
    "        - \"sept.\" was changed to \"Sep\"\n",
    "    - x30 contains day of the week.  \"thurday\" changed to \"thursday\"\n",
    "    - x32 contains a percentage amount. All % signs were removed and datatype changed to float64\n",
    "    - x37 contains a dollar amount.  All $ were removed and the datatype was changed to float64\n",
    "4.  Missing Value's \n",
    "    - Each column look to have anywhere from 21 - 47 missing values.  At most, this comprises about 2.9% of the data.  Which is a small amount when compared to the full dataset.   Instead of dropping those rows, we imputed with the median for each numeric column.\n",
    "5.  Distribution\n",
    "    - After checking the histograms for each column, it was determined the data was normally distributed with no skew.  This leads us to believe that this dataset could have been generated with sklearns make_classification function.  To see the distributions check Figure 1 below. \n",
    "6.  Scaling\n",
    "    - In order to make sure our data was scaled correctly for classification, we implemented sklearns StandardScaler function over the numerical columns.\n",
    "7.  Categorical features\n",
    "    - The continent variable was encoded in two ways, neither of which panned out, China or not china, and one hot encoded. All categorical variables were mode imputed, and all continuous variables were mean imputed. Mean imputation is appropriate because all the variables followed a normal distribution, as seen in the figure below:\n",
    "    \n",
    "![Histograms](../plots/Histograms.png)\n",
    "**Figure 1:** histograms of the numerical data\n",
    "Continuous distributions were tested both with and without scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "it turns out that EDA is a bit difficult when you have no idea what the corresponding columns don't have any \n",
    "\n",
    "# TODO TALK ABOUT CARSONS CYCLIC SHIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Selection\n",
    "Feature selection took our group quite some time as having unlabeled columns makes it difficult to quantify what variables are important.  When this is the case, we rely on other methods to determine what columns are important for analysis and going back and forth of dropping them and running our analysis on various models. \n",
    "\n",
    "\n",
    "\n",
    "Our first attempt at this process was using Random Forest which comes with feature importance.  Upon the first run, we saw a fairly low variable scoring of importance within the dataset. Due to this, we decided to implement a \"Random\" column in to the analysis to see if random data was indeed more useful than the data.  Luckily, as you can see in the Figure 2 below, the random column ranks fairly low in the importance so we can rest a little easier knowing that our data contains useful information.  \n",
    "\n",
    "### Random Forest Feature Importance\n",
    "\n",
    "![FeatImportance](../plots/BaselineRF_Feature_Imp.png \"Baseline Random Forest Feature Importance\") **Figure 2:** Feature importance of baseline Random Forest\n",
    "\n",
    "\n",
    "The most important features according to Random Forest > 0.02 were as follows. \n",
    "\n",
    "    - x23\n",
    "    - x20\n",
    "    - x48\n",
    "    - x49\n",
    "    - x42\n",
    "    - x12\n",
    "    - x28\n",
    "    - x27\n",
    "    - x40\n",
    "    - x37\n",
    "    - x7 \n",
    "    - x46 \n",
    "    - x41\n",
    "    - x38\n",
    "    - x2 \n",
    "    - x6 \n",
    "    - x32\n",
    "\n",
    "Leaving the remaining 33 variables available for dropping in re-running a random forest.  We didn't immediately drop these columns because we wanted to run Principal Component Analysis (PCA) on the full dataset to discover how many columns we needed to keep to maintain at least 95% variance. This type of dimensionality technique is very useful when you have unlabeled data such as this.  \n",
    "\n",
    "### Principal Component Analysis\n",
    "\n",
    "The main beneft of PCA is to reduce the number of features when computational cost becomes too cumbersome when running a model.  Luckily there's only 50 features for this dataset, but some can number in the 100's.  Below in Figure XX, we can see that in order to maintain 95% variance, we can select up to 36 different features to maintain our desired variance.  This is about twice as many features as with the random forest, but maintaining a conservative approach, we will use this limit to set our n_components=36 for the next random forest on the reduced dataset.  \n",
    "\n",
    "![PCA](../plots/PCA.png \"Principal Component Analysis\") **Figure 3:** PCA feature importance with respect to variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "Before we begin delving into classification models, its important to describe our loss function for this exercise as stipulated by our business partner.  \n",
    "\n",
    "#### Custom \"Slater-loss\" Function\n",
    "\n",
    "We now proceed with basic modeling of the data. The loss function is calculated as follows (given a confusion matrix $C$):\n",
    "$$\n",
    "\\mathbb{L}_\\mathrm{slater} =\\frac {C * \\begin{bmatrix}\n",
    "0 & 10  \\\\\n",
    "500 & 0  \\\\\n",
    "\\end{bmatrix}}  {\\sum_i \\sum_j C}\n",
    "$$\n",
    "Representing the cost in dollars per prediction. We want a dollar amount loss for every false positive/negative prediction. The business must earn more than that per prediction for this to be profitable.  We've come up with a way to evaluate the cost function using sklearns make_scorer [Make scorer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer).  To use this correctly, we will need to multiply the resulting confusion matrices by the weights we were given by Dr. Slater.  Below is the code for how this was performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    weight = np.array([[0, 10], [500, 0]])\n",
    "    out = cm * weight\n",
    "    return out.sum()/cm.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in the target columns true and predicted values, then multiplys the corresponding false positive/negatives by the dollar amount penalty for each.  It then divides the outcome by the sum of the confusion matrix to give us a dollar amount lost per incorrect prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Baseline Random Forest\n",
    "As was introduced it earlier, our baseline random forest was run on our dataset after it had been cleaned and prepped for analysis in the data cleaning section. We used an 80-20 test split and stratified the target variable, using a 5 fold cross validation.  Our base prediction of the Random Forest Model built 300 tree's yielding quite good results as shown below.\n",
    "\n",
    "Baseline Random Forrest:\n",
    "Accuracy of Baseline RF: 92.49 %\n",
    "\n",
    "Confusion Matrix:\n",
    " \n",
    "| 18374 | 787 |\n",
    "|-------|-------|\n",
    "| 1617 | 11222 |\n",
    "\n",
    "Custom Cross Validation Score:\n",
    "\n",
    "(22.96359944 24.6265625  20.7203125  20.00625    23.20518831)\n",
    "\n",
    " Classification Report\n",
    "             \n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      0.96      0.94     19161\n",
    "           1       0.93      0.87      0.90     12839\n",
    "\n",
    "    accuracy                           0.92     32000\n",
    "    macro avg      0.93      0.92      0.92     32000\n",
    "    weighted avg   0.93      0.92      0.92     32000\n",
    "\n",
    "As you can see from the custom cross validation scores, On average a false positive/negative only costs our business partner around $23.30 for each incorrect prediction.   \n",
    "\n",
    "# TODO - LINK CODE SECTION TO APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with Permutation Importance\n",
    "\n",
    "Next, we used permutation importance in order to determine features which are important for generalization. An excellent discussion of permutation importance and other importance tools can be seen in [this blog post by fast.ai](https://explained.ai/rf-importance/) and [this blog post by the authors](https://josephsdavid.github.io/iml.html). \n",
    "\n",
    "Accuracy of RF w permutation importance: 93.28 %\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "| 18355 | 806 |\n",
    "|-------|-------|\n",
    "| 1345 | 11494 |\n",
    "\n",
    "Custom Cross Validation Score:\n",
    "\n",
    "(22.55929058 21.17351849 -19.50295614 -20.18493057 -20.40770024)\n",
    "\n",
    "Classification Report\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.93      0.96      0.94     19161\n",
    "           1       0.93      0.89      0.91     12839\n",
    "\n",
    "    accuracy                           0.93     32000\n",
    "    macro avg      0.93      0.93      0.92     32000\n",
    "    weighted avg   0.93      0.93      0.93     32000\n",
    "\n",
    "This model did very well showing our business partner a cost of $20.77 for each incorrect prediction. \n",
    "\n",
    "# TODO - LINK CODE SECTION TO APPENDIX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with Prinicipal Component Analysis\n",
    "\n",
    "We introduced PCA in the feature engineering section of this case study but will now implement another random forest with reduced dimensions set at n_components = 36.  As our earlier chart showed that would be a proper amount of features to select to maintain our goal of 95% of the variance.\n",
    "\n",
    "Accuracy of RF w PCA: 83.25 %\n",
    "Confusion Matrix:\n",
    "\n",
    "| 17496 | 1665 |\n",
    "|-------|------|\n",
    "| 3695 | 9144 |\n",
    "\n",
    "Custom Cross Validation Score:\n",
    "\n",
    "(33.12294954 33.0703125  28.484375   34.0953125  32.66760431)\n",
    "\n",
    "Classification Report\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.91      0.87     19161\n",
    "           1       0.85      0.71      0.77     12839\n",
    "\n",
    "    accuracy                           0.83     32000\n",
    "    macro avg      0.84      0.81      0.82     32000\n",
    "    weighted avg   0.83      0.83      0.83     32000\n",
    "\n",
    "Here we see the classification accuracy lower 9% points lower and the cost function yielding an average of \\$32.29 per wrong prediction.  Seeing as our business partner wants to save money rather than spend more, we don't want him/her to want to pay \\$10 more per wrong prediction.  We don't suggest dimensionality reduction to optimize cost savings.  \n",
    "\n",
    "\n",
    "Next, we implement a Logistic Regression Model as its another popular model for classification.  \n",
    "\n",
    "\n",
    "# TODO - LINK CODE SECTION TO APPENDIX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Now that we've seen a few models perform, lets turn our attention to logistic regression.  Our business partner has predicted that one might be able to achieve excellent accuracies with this particular model so we're interested in its implementation.  \n",
    "\n",
    "\n",
    "Baseline Logistic Regression:\n",
    "Accuracy of Logistic Regression: 70.44 %\n",
    "Confusion Matrix:\n",
    "\n",
    "| 1590 | 3260 |\n",
    "|------|------|\n",
    "| 6200 | 6639 |\n",
    "\n",
    "Custom Cross Validation Score:\n",
    "\n",
    "(97.47509863 97.85672435 98.18398437 96.64205633 99.56365483)\n",
    "\n",
    "Classification Report\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "           0       0.72      0.83      0.77     19161\n",
    "           1       0.67      0.52      0.58     12839\n",
    "\n",
    "    accuracy                           0.70     32000\n",
    "    macro avg      0.70      0.67      0.68     32000\n",
    "    weighted avg   0.70      0.70      0.70     32000\n",
    "\n",
    "\n",
    "Sadly our initial thoughts on this being an overly successful algorithm with the dataset is not working as expected.  Accuracy has dropped to 70.34% and on average, our business partner is losing almost \\$100 for every wrong prediction.  With that being said, Its probably safe to say that tuning a logistic regression at this point isn't going to recover 22% points in accuracy in order to catch up to Random Forest.  \n",
    "\n",
    "# TODO - LINK CODE SECTION TO APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree's Classifier\n",
    "\n",
    "Lastly, we'll try implementing a Extra Tree's Classifer see if we can improve upon our baseline random forest.  The advantage of Extra Tree's is that this algorithm fits a number of randomized tree's on sub-samples of the dataset.  Using averaging to improve predictive accuracy and prevent over-fitting.  [Extra Tree's Docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)\n",
    "\n",
    "Baseline Extra Random Forest:\n",
    "Accuracy of Baseline ERF: 92.38 %\n",
    "\n",
    "Confusion Matrix: \n",
    "\n",
    "| 18473 | 688 |\n",
    "|-------|-------|\n",
    "| 1750 | 11089 |\n",
    "\n",
    "Custom Cross Validation Score:\n",
    "\n",
    "(30.3953125 32.35      25.8765625 27.7890625 29.534375)\n",
    "\n",
    "Classification Report\n",
    "\n",
    "                precision    recall  f1-score   support\n",
    "           0       0.91      0.96      0.94     19161\n",
    "           1       0.94      0.86      0.90     12839\n",
    "    accuracy                           0.92     32000\n",
    "    macro avg      0.93      0.91      0.92     32000\n",
    "    weighted avg   0.92      0.92      0.92     32000\n",
    "\n",
    "# TODO - LINK CODE SECTION TO APPENDIX\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Between all the models we've run, it seems the untuned random forest performed best barely beating out Extra Tree's Classifier by 0.01%.  Dimensionality reduction with PCA and another Random Forrest didn't pan out well for our purposes, while Logistic Regression came in last place in terms of accuracy and cost savings.  Therefore our suggestion to our business partner is stick with Random Forest for their classification needs in cost savings.  \n",
    "\n",
    "| Model                                     \t| Accuracy \t| Custom Scoring Loss \t|\n",
    "|-------------------------------------------\t|----------\t|---------------------\t|\n",
    "| Random Forest with Permutation Importance \t| 93.28%   \t| $20.77              \t|\n",
    "| Random Forest                             \t| 92.49%   \t| $23.30              \t|\n",
    "| Extra Tree's                              \t| 92.48%   \t| $29.19              \t|\n",
    "| Random Forest with PCA                    \t| 83.25%   \t| $32.29              \t|\n",
    "| Logistic Regression                       \t| 70.44%   \t| $97.94              \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code Appendix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}